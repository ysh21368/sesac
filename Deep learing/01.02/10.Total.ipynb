{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2a4ab8a-270c-47b3-b7e3-4ded57bae73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic1 = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9f733bb-f725-4994-acfd-24b21e97bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet :\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01 ): #input 입력층(최초), hidden 은닉층, output 출력층(결과), \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) # 입력층과. 은닉층에 랜덤 값을 준다. ex) 강아지라고 하면 강아지 사진 1,2,3\n",
    "        self.params['b1'] = np.zeros(hidden_size) # 은닉층\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size) # 출력층\n",
    "        self.layers = OrderedDict() # 딕셔너리를 for문을 돌릴수 있도록 도와줌\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1']) #9장에 있음. #W1, b1을 넣는다.\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayers = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict (self, x) :\n",
    "        for layer in self.layers.values() :\n",
    "            x = layer.forward(x) # 예측 단계에서는 softmax함수가 어차피 실제값과 같기 때문에 필요없음.(argmax)\n",
    "                                # 단, 학습단계에서는 loss값을 구해야 하기 때문에 softmax적용이 필요.\n",
    "        return x\n",
    "    def loss(self, x, t) :\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayers.forward(y,t)\n",
    "    \n",
    "    def accuracy(self, x, t) :\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / x.shape[0]\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda _: self.loss(x,t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    def gradient(self, x, t) :\n",
    "        self.loss(x,t)\n",
    "        dout = 1\n",
    "        dout = self.lastLayers.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers :\n",
    "            dout = layer.backward(dout)\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46966ecc-99e5-4a0a-89a4-d43700674adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "from util.layers import *\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf05e66e-e779-48fe-9350-14921fdec9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8466cee-ce47-4227-8ec7-b376b57434c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9dd8ba0-928d-4eb8-bc1c-05265929d9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (784, 50) (784, 50)\n",
      "W1:2.525640416035734e-13\n",
      "b1 (50,) (50,)\n",
      "b1:9.019682787117511e-13\n",
      "W2 (50, 10) (50, 10)\n",
      "W2:9.416976019552091e-13\n",
      "b2 (10,) (10,)\n",
      "b2:1.2012612987666316e-10\n"
     ]
    }
   ],
   "source": [
    "for key in grad_numerical.keys() :\n",
    "    print(key, grad_backprop[key].shape, grad_numerical[key].shape)\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d31145a-27b6-4a36-bb69-fc32caa6846b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                            | 133/10000 [00:00<00:16, 582.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.10861666666666667, 0.112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████▉                                                                       | 770/10000 [00:00<00:11, 816.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9028833333333334, 0.9086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████▎                                                                 | 1360/10000 [00:01<00:10, 821.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9212166666666667, 0.9212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████▉                                                             | 1971/10000 [00:02<00:09, 808.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9346, 0.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████▎                                                        | 2538/10000 [00:03<00:09, 802.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9434666666666667, 0.9409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████▍                                                   | 3218/10000 [00:03<00:08, 786.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9516666666666667, 0.9499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████▏                                               | 3710/10000 [00:04<00:08, 720.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9551666666666667, 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|█████████████████████████████████▌                                          | 4414/10000 [00:05<00:06, 802.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9614, 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████                                      | 5004/10000 [00:05<00:06, 772.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9633166666666667, 0.9601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|██████████████████████████████████████████▌                                 | 5593/10000 [00:06<00:05, 810.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.96605, 0.9616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████████████████████████████████████████████▋                             | 6142/10000 [00:07<00:05, 768.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9697, 0.9657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████                         | 6713/10000 [00:07<00:04, 769.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9715166666666667, 0.9669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████████████████████████████▌                    | 7310/10000 [00:08<00:03, 809.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9727, 0.9656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|████████████████████████████████████████████████████████████▎               | 7942/10000 [00:09<00:02, 819.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9725333333333334, 0.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████████████████████████████████████▎          | 8599/10000 [00:10<00:01, 862.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9765666666666667, 0.9681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████████████████      | 9226/10000 [00:10<00:00, 838.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9785166666666667, 0.9691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████▍ | 9791/10000 [00:11<00:00, 796.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9797, 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:11<00:00, 859.36it/s]\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True) #값이 \n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10) \n",
    "\n",
    "train_loss_list = []\n",
    "#추가\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "lerning_rate = 0.1\n",
    "\n",
    "#추가\n",
    "iter_per_epoch = train_size / batch_size\n",
    "\n",
    "\n",
    "for i in tqdm(range(iters_num)):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch,t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1','W2', 'b2') :\n",
    "        network.params[key] -= lerning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc |\" + str(train_acc) + \", \"+ str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "efc2bc9f-13a7-4c5a-a852-655c6610d71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7079cb-23c5-4c56-9e9f-5bfd007a5541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d37c6-2d39-4d2b-9721-063190a3fc19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
